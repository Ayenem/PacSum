@startuml
package bert_model{
 class BertModel{}
}

package extractor{

class PacSumExtractor{
 +extract_num
 +beta 
 +lambda1 
 +lambda2 

 +extract_summary()
 +tune_hparams()
 -_calculate_similarity_matrix()
 -_select_tops()
 -_tune_extractor()
}
PacSumExtractor --> BertModel

class PacSumExtractorWithBert extends PacSumExtractor {
 +model

 -_calculate_similarity_matrix()
 -_generate_score()
 -_load_edge_model()
}

class PacSumExtractorWithTfIdf extends PacSumExtractor {
 -_calculate_similarity_matrix()
 -_idf_modified_dot()
 -_calculate_idf_scores()
}

}

package tokenizer{

class tokenizer_<< globals >>{
 convert_to_unicode()
 printable_text()
 load_vocab()
 convert_tokens_to_ids()
 whitespace_tokenize()
 _is_whitespace()
 _is_control()
 _is_punctuation()

}

class BasicTokenizer {
 +do_lower_case 

 +tokenize()
 -_run_strip_accents()
 -_run_split_on_punc()
 -_tokenize_chinese_chars()
 -_is_chinese_char()
 -_clean_text()
}

class FullTokenizer {
 +vocab
 +basic_tokenizer
 +wordpiece_tokenizer

 +tokenize()
 +convert_tokens_to_ids()
}


class WordpieceTokenizer {
 +vocab
 +unk_token
 +max_input_chars_per_word

 +tokenize()
}

}

package data_iterator{

class Dataset{
 -_file_pattern
 -_max_len
 -_tokenizer

 +iterate_once_doc_tfidf()
 -_doc_stream_tfidf()
 -_parse_file2doc_tfidf()
 +iterate_once_doc_bert()
 -_doc_stream_bert()
 -_parse_file2doc_bert()
 -_doc_iterate_bert()
 -_2bert_rep()
 -_truncate_seq_pair()
}
Dataset o-- FullTokenizer

}

class run<< Entry Point >>{
 args
 parser
}
run --> Dataset
run --> PacSumExtractorWithBert
run --> PacSumExtractorWithTfIdf
@enduml
